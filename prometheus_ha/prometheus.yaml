apiVersion: template.openshift.io/v1
kind: Template

metadata:
  name: prometheus_ha
  annotations:
    "openshift.io/display-name": Prometheus_ha
    description: |
      A monitoring solution to get metrics and alerts for applications running in a namespace.
    iconClass: icon-cogs
    tags: "monitoring,prometheus,time-series"

parameters:
- description: The namespace where to deploy the Prometheus service
  name: NAMESPACE
  required: true
- description: The Prometheus service account (must be created beforehand)
  name: PROMETHEUS_SA
  value: prometheus
- description: The location of the Prometheus image
  name: IMAGE_PROMETHEUS
  value: openshift/prometheus:v2.0.0
- description: The location of the AlertManager image
  name: IMAGE_ALERTMANAGER
  value: openshift/prometheus-alertmanager:v0.9.1
- description: The location of the proxy image
  name: IMAGE_PROXY
  value: openshift/oauth-proxy:v1.0.0
- description: The location of alert-buffer image
  name: IMAGE_ALERTBUFFER
  value: openshift/prometheus-alert-buffer:v0.0.2
- description: The secret for the AlertManager
  name: ALERTMANAGER_SECRET
  generate: expression
  from: "[a-zA-Z0-9]{43}"
- description: The session secret for the proxy
  name: PROXY_SECRET
  generate: expression
  from: "[a-zA-Z0-9]{43}"

objects:
- apiVersion: v1
  kind: Service
  metadata:
    name: prometheus
    namespace: "${NAMESPACE}"
    annotations:
      service.alpha.openshift.io/serving-cert-secret-name: prometheus-tls
  spec:
    ports:
    - name: prometheus
      port: 443
      protocol: TCP
      targetPort: 8443
    selector:
      app: prometheus

- apiVersion: v1
  kind: Service
  metadata:
    name: alertmanager
    namespace: "${NAMESPACE}"
    annotations:
      service.alpha.openshift.io/serving-cert-secret-name: alertmanager-tls
  spec:
    ports:
    - name: alertmanager
      port: 443
      protocol: TCP
      targetPort: 9443
    selector:
      app: prometheus

- apiVersion: v1
  kind: Service
  metadata:
    name: alertbuffer
    namespace: "${NAMESPACE}"
    annotations:
      service.alpha.openshift.io/serving-cert-secret-name: alertbuffer-tls
  spec:
    ports:
    - name: alertbuffer
      port: 443
      protocol: TCP
      targetPort: 8443
    selector:
      app: alertbuffer

- apiVersion: route.openshift.io/v1
  kind: Route
  metadata:
    name: prometheus
    namespace: "${NAMESPACE}"
  spec:
    to:
      name: prometheus
    tls:
      termination: Reencrypt
      insecureEdgeTerminationPolicy: Redirect

- apiVersion: route.openshift.io/v1
  kind: Route
  metadata:
    name: alertmanager
    namespace: "${NAMESPACE}"
  spec:
    to:
      name: alertmanager
    tls:
      termination: Reencrypt
      insecureEdgeTerminationPolicy: Redirect

- apiVersion: route.openshift.io/v1
  kind: Route
  metadata:
    name: alertbuffer
    namespace: "${NAMESPACE}"
  spec:
    to:
      name: alertbuffer
    tls:
      termination: Reencrypt
      insecureEdgeTerminationPolicy: Redirect

- apiVersion: v1
  kind: Secret
  metadata:
    name: proxy
    namespace: "${NAMESPACE}"
  stringData:
    session_secret: "${PROXY_SECRET}="

- apiVersion: apps/v1beta1
  kind: StatefulSet
  metadata:
    labels:
      app: prometheus
    name: prometheus
    namespace: "${NAMESPACE}"
  spec:
    selector:
      matchLabels:
        app: prometheus
    updateStrategy:
      type: RollingUpdate
    podManagementPolicy: Parallel
    replicas: 2
    serviceName: prometheus
    template:
      metadata:
        labels:
          app: prometheus
        name: prometheus
      spec:
        serviceAccountName: "${PROMETHEUS_SA}"
        containers:
        - name: proxy
          image: ${IMAGE_PROXY}
          imagePullPolicy: IfNotPresent
          ports:
          - containerPort: 8443
            name: web
          args:
          - -provider=openshift
          - -https-address=:8443
          - -http-address=
          - -email-domain=*
          - -upstream=http://localhost:9090
          - -client-id=system:serviceaccount:${NAMESPACE}:${PROMETHEUS_SA}
          - -openshift-ca=/etc/pki/tls/cert.pem
          - -openshift-ca=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          - '-openshift-sar={"resource": "namespaces", "verb": "get", "resourceName": "${NAMESPACE}", "namespace": "${NAMESPACE}"}'
          - '-openshift-delegate-urls={"/": {"resource": "namespaces", "verb": "get", "resourceName": "${NAMESPACE}", "namespace": "${NAMESPACE}"}}'
          - -tls-cert=/etc/tls/private/tls.crt
          - -tls-key=/etc/tls/private/tls.key
          - -client-secret-file=/var/run/secrets/kubernetes.io/serviceaccount/token
          - -cookie-secret-file=/etc/proxy/secrets/session_secret
          volumeMounts:
          - mountPath: /etc/tls/private
            name: prometheus-tls
          - mountPath: /etc/proxy/secrets
            name: proxy-secret
        - name: prometheus
          args:
          - --storage.tsdb.retention=24h
          - --config.file=/etc/prometheus/prometheus.yml
          - --web.enable-admin-api
          - --log.level=debug
          image: ${IMAGE_PROMETHEUS}
          imagePullPolicy: IfNotPresent
          volumeMounts:
          - mountPath: /etc/prometheus
            name: prometheus-config
          - mountPath: /prometheus
            name: prometheus-data
          readinessProbe:
            httpGet:
              path: /-/ready
              port: 9090
            initialDelaySeconds: 10
          livenessProbe:
            httpGet:
              path: /-/healthy
              port: 9090
            initialDelaySeconds: 10
          ports:
          - containerPort: 9090
            name: web
            protocol: TCP
        - name: am-proxy
          image: ${IMAGE_PROXY}
          imagePullPolicy: IfNotPresent
          ports:
          - containerPort: 9443
            name: web
          args:
          - -provider=openshift
          - -https-address=:9443
          - -http-address=
          - -email-domain=*
          - -upstream=http://localhost:9093
          - -client-id=system:serviceaccount:${NAMESPACE}:${PROMETHEUS_SA}
          - -openshift-ca=/etc/pki/tls/cert.pem
          - -openshift-ca=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          - '-openshift-sar={"resource": "namespaces", "verb": "get", "resourceName": "${NAMESPACE}", "namespace": "${NAMESPACE}"}'
          - '-openshift-delegate-urls={"/": {"resource": "namespaces", "verb": "get", "resourceName": "${NAMESPACE}", "namespace": "${NAMESPACE}"}}'
          - -tls-cert=/etc/tls/private/tls.crt
          - -tls-key=/etc/tls/private/tls.key
          - -client-secret-file=/var/run/secrets/kubernetes.io/serviceaccount/token
          - -cookie-secret-file=/etc/proxy/secrets/session_secret
          volumeMounts:
          - mountPath: /etc/tls/private
            name: alertmanager-tls
          - mountPath: /etc/proxy/secrets
            name: proxy-secret
        - name: alertmanager
          args:
          - --mesh.listen-address=:6783
          - --mesh.peer=prometheus-0.prometheus.${NAMESPACE}.endpoints:6783
          - --mesh.password=${ALERTMANAGER_SECRET}
          - --config.file=/etc/alertmanager/alertmanager.yml
          - --log.level=debug
          image: "${IMAGE_ALERTMANAGER}"
          imagePullPolicy: IfNotPresent
          ports:
          - containerPort: 9093
            name: web
            protocol: TCP
          volumeMounts:
          - mountPath: /etc/alertmanager
            name: alertmanager-config
          - mountPath: /alertmanager
            name: alertmanager-data
        restartPolicy: Always
        volumes:
        - name: proxy-secret
          secret:
            secretName: proxy
        - name: prometheus-config
          configMap:
            defaultMode: 420
            name: prometheus-config
        - name: prometheus-data
          emptyDir: {}
        - name: prometheus-tls
          secret:
            secretName: prometheus-tls
        - name: alertmanager-config
          configMap:
            defaultMode: 420
            name: alertmanager-config
        - name: alertmanager-data
          emptyDir: {}
        - name: alertmanager-tls
          secret:
            secretName: alertmanager-tls

- apiVersion: v1
  kind: ConfigMap
  metadata:
    name: prometheus-config
    namespace: "${NAMESPACE}"
  data:
    prometheus.rules: |
      groups:
      - name: prometheus
        interval: 30s
        rules:
        - alert: Prometheus down
          expr: up{job="prometheus"} == 0 or absent(up{job="prometheus"})
          for: 1m
          labels:
            severity: "Critical"
          annotations:
            message: "Prometheus instance {{ $labels.instance }} is down."
        - alert: AlertManager down
          expr: up{job="alertmanager"} == 0 or absent(up{job="alertmanager"})
          for: 1m
          labels:
            severity: "Critical"
          annotations:
            message: "AlertManager instance {{ $labels.instance }} is down."
        - alert: AlertManager not discovered
          expr: prometheus_notifications_alertmanagers_discovered < 2
          for: 1m
          labels:
            severity: "Critical"
          annotations:
            message: "Prometheus instance {{ $labels.instance }} has discovered only {{ $value }} AlertManager instances."
        - alert: Scrapes with rejected samples
          expr: increase(prometheus_target_scrapes_exceeded_sample_limit_total[5m]) > 0
          for: 5m
          labels:
            severity: "Warning"
          annotations:
            message: "Prometheus instance {{ $labels.instance }} has scraped {{ $value }} targets exceeding the sample limit in the last 5 minutes."
        - alert: WAL corruption
          expr: increase(tsdb_wal_corruptions_total[5m]) > 0
          for: 5m
          labels:
            severity: "Critical"
          annotations:
            message: "Prometheus instance {{ $labels.instance }} has encountered {{ $value }} WAL corruptions in the last 5 minutes."
        - alert: Compaction failures
          expr: increase(prometheus_tsdb_compactions_failed_total[5m]) > 0
          for: 5m
          labels:
            severity: "Warning"
          annotations:
            message: "Prometheus instance {{ $labels.instance }} has encountered {{ $value }} compaction failures in the last 5 minutes."
        - alert: TSDB reload failures
          expr: rate(prometheus_tsdb_reloads_failures_total[5m]) / rate(prometheus_tsdb_reloads_total[5m]) * 100 > 5
          for: 5m
          labels:
            severity: "Critical"
          annotations:
            message: "Prometheus instance {{ $labels.instance }} has encountered {{ $value }}% of block reload failures in the last 5 minutes."
        - alert: File SD errors
          expr: rate(prometheus_sd_file_read_errors_total[5m]) / rate(prometheus_sd_file_scan_duration_seconds_count[5m]) * 100 > 5
          for: 5m
          labels:
            severity: "Warning"
          annotations:
            message: "Prometheus instance {{ $labels.instance }} has encountered {{ $value }}% of read errors for the file SD in the last 5 minutes."
        - alert: Rule evaluation failures
          expr: rate(prometheus_rule_evaluation_failures_total[5m]) / rate(prometheus_rule_evaluation_duration_seconds_count[5m]) * 100 > 5
          for: 5m
          labels:
            severity: "Warning"
          annotations:
            message: "Prometheus instance {{ $labels.instance }} has encountered {{ $value }}% of evaluation errors in the last 5 minutes."
        # The following alerts may not reach the final receiver(s) since they fire when AlertManager isn't working properly
        - alert: Alert notification errors
          expr: rate(prometheus_notifications_errors_total[5m]) / rate(prometheus_notifications_sent_total[5m]) * 100 > 5
          for: 5m
          labels:
            severity: "Warning"
          annotations:
            message: "Prometheus instance {{ $labels.instance }} has encountered {{ $value }}% of alert notification errors in the last 5 minutes."
        - alert: Invalid alerts
          expr: rate(alertmanager_alerts_invalid_total[5m]) / sum(rate(alertmanager_alerts_received_total[5m])) without (status) * 100 > 5
          for: 5m
          labels:
            severity: "Warning"
          annotations:
            message: "Alertmanager instance {{ $labels.instance }} has encountered {{ $value }}% of invalid alerts in the last 5 minutes."
        - alert: Failed notifications
          expr: rate(alertmanager_notifications_failed_total[5m]) / rate(alertmanager_notifications_total[5m]) * 100 > 5
          for: 5m
          labels:
            severity: "Warning"
          annotations:
            message: "Receiver {{ $labels.receiver }} on Alertmanager instance {{ $labels.instance }} has encountered {{ $value }}% of failures in the last 5 minutes."
        - alert: Silence query errors
          expr: rate(alertmanager_silences_query_errors_total[5m]) / rate(alertmanager_silences_queries_total[5m]) * 100 > 5
          for: 5m
          labels:
            severity: "Warning"
          annotations:
            message: "Alertmanager instance {{ $labels.instance }} has encountered {{ $value }}% of errors on silence queries in the last 5 minutes."
        - alert: Notification log query errors
          expr: rate(alertmanager_nflog_query_errors_total[5m]) / rate(alertmanager_nflog_queries_total[5m]) * 100 > 5
          for: 5m
          labels:
            severity: "Warning"
          annotations:
            message: "Alertmanager instance {{ $labels.instance }} has encountered {{ $value }}% of errors on notification log queries in the last 5 minutes."

      - name: endpoints
        interval: 30s
        rules:
        - alert: Endpoint down
          expr: up{job="endpoints"} == 0
          for: 1m
          labels:
            severity: "Warning"
          annotations:
            message: "Prometheus instance {{ $labels.instance }} is down."

    prometheus.yml: |
      global:
        scrape_interval: 10s

      rule_files:
        - 'prometheus.rules'

      alerting:
        alertmanagers:
        - scheme: https
          tls_config:
            # the certificate is generated for the service, not the endpoint itself
            insecure_skip_verify: true
          bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

          kubernetes_sd_configs:
          - role: endpoints
            namespaces:
              names:
              - "${NAMESPACE}"

          relabel_configs:
          - source_labels:
            - __meta_kubernetes_service_name
            action: keep
            regex: alertmanager

      scrape_configs:
      - job_name: 'prometheus'
        scheme: https
        tls_config:
          # the certificate is generated for the service, not the endpoint itself
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

        kubernetes_sd_configs:
        - role: endpoints
          namespaces:
            names:
            - "${NAMESPACE}"

        relabel_configs:
        # metrics are pulled through the OAuth proxy container
        - source_labels:
          - __meta_kubernetes_service_name
          - __meta_kubernetes_pod_container_name
          action: keep
          regex: prometheus;proxy
        - source_labels: [__meta_kubernetes_pod_name]
          action: replace
          target_label: instance

      - job_name: 'alertmanager'
        scheme: https
        tls_config:
          # the certificate is generated for the service, not the endpoint itself
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

        kubernetes_sd_configs:
        - role: endpoints
          namespaces:
            names:
            - "${NAMESPACE}"

        relabel_configs:
        - source_labels:
          - __meta_kubernetes_service_name
          - __meta_kubernetes_pod_container_name
          action: keep
          regex: alertmanager;am-proxy
        - source_labels: [__meta_kubernetes_pod_name]
          action: replace
          target_label: instance

      - job_name: 'alertbuffer'
        scheme: https
        tls_config:
          # the certificate is generated for the service, not the endpoint itself
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

        kubernetes_sd_configs:
        - role: endpoints
          namespaces:
            names:
            - "${NAMESPACE}"

        relabel_configs:
        - source_labels:
          - __meta_kubernetes_service_name
          - __meta_kubernetes_pod_container_name
          action: keep
          regex: alertbuffer;proxy
        - source_labels: [__meta_kubernetes_pod_name]
          action: replace
          target_label: instance

      # See the namespaced_kube-state-metrics directory
      - job_name: 'kube-state-metrics'
        scheme: http

        kubernetes_sd_configs:
        - role: pod
          namespaces:
            names:
            - "${NAMESPACE}"

        relabel_configs:
        - source_labels:
          - __meta_kubernetes_pod_container_name
          action: keep
          regex: kube-state-metrics

      - job_name: 'endpoints'
        tls_config:
          insecure_skip_verify: true

        kubernetes_sd_configs:
        - role: endpoints
          namespaces:
            names:
            - "${NAMESPACE}"

        relabel_configs:
        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
          action: keep
          regex: true
        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
          action: replace
          target_label: __scheme__
          regex: (https?)
        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
          action: replace
          target_label: __address__
          regex: (.+)(?::\d+);(\d+)
          replacement: $1:$2
        - action: labelmap
          regex: __meta_kubernetes_service_label_(.+)
        - source_labels: [__meta_kubernetes_service_name]
          action: replace
          target_label: service_name

- apiVersion: apps/v1beta1
  kind: StatefulSet
  metadata:
    name: alertbuffer
    namespace: "${NAMESPACE}"
  spec:
    selector:
      matchLabels:
        app: alertbuffer
    updateStrategy:
      type: RollingUpdate
    podManagementPolicy: Parallel
    serviceName: alertbuffer
    replicas: 1
    template:
      metadata:
        labels:
          app: alertbuffer
        name: alertbuffer
      spec:
        serviceAccountName: "${PROMETHEUS_SA}"
        containers:
        - name: proxy
          image: ${IMAGE_PROXY}
          imagePullPolicy: IfNotPresent
          ports:
          - containerPort: 8443
            name: web
          args:
          - -provider=openshift
          - -https-address=:8443
          - -http-address=
          - -email-domain=*
          - -upstream=http://localhost:9099
          - -client-id=system:serviceaccount:${NAMESPACE}:${PROMETHEUS_SA}
          - -openshift-ca=/etc/pki/tls/cert.pem
          - -openshift-ca=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          - '-openshift-sar={"resource": "namespaces", "verb": "get", "resourceName": "${NAMESPACE}", "namespace": "${NAMESPACE}"}'
          - '-openshift-delegate-urls={"/": {"resource": "namespaces", "verb": "get", "resourceName": "${NAMESPACE}", "namespace": "${NAMESPACE}"}}'
          - -tls-cert=/etc/tls/private/tls.crt
          - -tls-key=/etc/tls/private/tls.key
          - -client-secret-file=/var/run/secrets/kubernetes.io/serviceaccount/token
          - -cookie-secret-file=/etc/proxy/secrets/session_secret
          volumeMounts:
          - mountPath: /etc/tls/private
            name: alertbuffer-tls
          - mountPath: /etc/proxy/secrets
            name: proxy-secret
        - name: alertbuffer
          args:
          - --storage-path=/alertbuffer/messages.db
          - --listen-address=0.0.0.0:9099
          image: "${IMAGE_ALERTBUFFER}"
          imagePullPolicy: IfNotPresent
          ports:
          - containerPort: 9099
            name: web
            protocol: TCP
          volumeMounts:
          - mountPath: /alertbuffer
            name: alertbuffer-data

        volumes:
        - name: alertbuffer-data
          emptyDir: {}
        - name: alertbuffer-tls
          secret:
            secretName: alertbuffer-tls
        - name: proxy-secret
          secret:
            secretName: proxy

- apiVersion: v1
  kind: ConfigMap
  metadata:
    label:
      app: prometheus
    name: alertmanager-config
    namespace: "${NAMESPACE}"
  data:
    alertmanager.yml: |
      global:

      # The root route on which each incoming alert enters.
      route:
        # Send all alerts to alert-buffer and group by job & severity
        receiver: alertbuffer-wh
        group_by: [job, severity]

      receivers:
      - name: alertbuffer-wh
        webhook_configs:
        - url: http://alertbuffer-0.alertbuffer.myproject.endpoints:9099/topics/alerts
# The following parameters aren't yet supported by AlertManager
#          bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
#          tls_config:
#            # the certificate is generated for the service, not the endpoint itself
#            insecure_skip_verify: true
